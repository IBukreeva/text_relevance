{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "import re\n",
    "import pymorphy2\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "import codecs\n",
    "import bs4\n",
    "from pymystem3 import Mystem\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Лемматизация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Эти ячейки не выполнены, потому что я один раз сделала лемматизацию в кегле и скачала, и все время работала с пиклами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "dires = ['20170726',\n",
    " '20170707',\n",
    " '20170708',\n",
    " '20170704',\n",
    " '20170710',\n",
    " '20170717',\n",
    " '20170702',\n",
    " '20170711'\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = \"/kaggle/input/text-relevance-competition-ir-1-ts-fall-2020/content/content/\"\n",
    "# path_short = \"/kaggle/input/text-relevance-competition-ir-1-ts-fall-2020/\"\n",
    "# pathtemp = \"/kaggle/working/\"\n",
    "path = \"\"\n",
    "path_short = \"\"\n",
    "pathtemp = \"\"\n",
    "\n",
    "NDOCS = 38114\n",
    "stem = Mystem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stop_my = '''которых которые твой которой которого сих ком свой твоя этими слишком нами всему будь саму чаще ваше сами наш затем еще самих наши ту каждое мочь весь этим наша своих оба который зато те этих вся ваш такая теми ею которая нередко\n",
    "каждая также чему собой самими нем вами ими откуда такие тому та очень сама нему алло оно этому кому тобой таки твоё\n",
    "каждые твои мой нею самим ваши ваша кем мои однако сразу свое ними всё неё тех хотя всем тобою тебе одной другие\n",
    "эта о само эта буду поэтому самой моё своей такое всею будут своего кого свои мог нам особенно её самому наше кроме вообще вон\n",
    "мною никто это'''\n",
    "stop_words = stopwords.words('russian')\n",
    "stop_words.extend(stop_my.split())\n",
    "len(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_info = defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatizeDocs(urls, titles, texts): # разделитель между текстами br!\n",
    "    number = titles.count(\"br!\")\n",
    "    \n",
    "    titles_l = \"\".join(stem.lemmatize(titles)).strip()\n",
    "    texts_l = \"\".join(stem.lemmatize(texts)).strip()\n",
    "    \n",
    "    for url, title, text in zip(urls, titles_l.split(\"br!\"), texts_l.split(\"br!\")):\n",
    "        docs_info[url] = [title, text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for dir in dires:\n",
    "\n",
    "    alltitles = \"\"\n",
    "    alltexts = \"\"\n",
    "    urls_list = []\n",
    "    for filename in tqdm(listdir(path + dir)):\n",
    "            \n",
    "        with codecs.open(path + dir + '/' + filename, 'r', 'utf-8', errors='ignore') as file:\n",
    "            url = file.readline().strip('\\n')\n",
    "            html = file.read()\n",
    "        \n",
    "        soup = BeautifulSoup(html)\n",
    "        \n",
    "        [s.extract() for s in soup(['script', 'style', 'title'])]\n",
    "        text = soup.get_text('\\n', True).lower()\n",
    "        text = re.compile(r'[A-Za-zА-Яа-я0-9]+').findall(text)\n",
    "        alltexts += ' '.join([e for e in text if e not in stop_words]) + \" br! \"\n",
    "    \n",
    "        soup = BeautifulSoup(html)\n",
    "        title = ' '.join([ e.get_text() for e in soup.find_all('title') if e.get_text() not in stop_words ]).lower()\n",
    "        title = re.compile(r'[A-Za-zА-Яа-я0-9]+').findall(title)\n",
    "        alltitles += ' '.join(title) + \" br! \" \n",
    "        \n",
    "        urls_list.append(url)\n",
    "        \n",
    "        if i%10==0: #10 текстов\n",
    "            lemmatizeDocs(urls_list, alltitles, alltexts)\n",
    "            urls_list = []\n",
    "            alltitles = \"\"\n",
    "            alltexts = \"\"\n",
    "    \n",
    "    lemmatizeDocs(urls_list, alltitles, alltexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "some_file = open(pathtemp+\"docs_info.pickle\", \"wb\")\n",
    "pickle.dump(docs_info, some_file)\n",
    "some_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatizeHeaders(urls, headers):\n",
    "    number = headers.count(\"br!\")\n",
    "    \n",
    "    headers_l = \"\".join(stem.lemmatize(headers)).strip()\n",
    "    \n",
    "    for url, head in zip(urls, headers_l.split(\"br!\")):\n",
    "        Headers[url] = head "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Headers = defaultdict(str)\n",
    "for dir in dires:\n",
    "\n",
    "    allheaders = \"\"\n",
    "    urls_list = []\n",
    "    for filename in tqdm(listdir(path + dir)):\n",
    "\n",
    "        with codecs.open(path + dir + '/' + filename, 'r', 'utf-8', errors='ignore') as file:\n",
    "            url = file.readline().strip('\\n')\n",
    "            html = file.read()\n",
    "        \n",
    "        soup = BeautifulSoup(html)\n",
    "        \n",
    "        header = ' '.join([e.get_text() for e in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])]).lower()\n",
    "        header = re.compile(r'[A-Za-zА-Яа-я0-9]+').findall(header)\n",
    "        allheaders += ' '.join(header) + \" br! \" \n",
    "        \n",
    "        urls_list.append(url)\n",
    "\n",
    "        if i%100==0:\n",
    "            lemmatizeHeaders(urls_list, allheaders)\n",
    "            urls_list = []\n",
    "            allheaders = \"\"\n",
    "    \n",
    "    lemmatizeHeaders(urls_list, allheaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "some_file = open(pathtemp+\"headers.pickle\", \"wb\")\n",
    "pickle.dump(Headers, some_file)\n",
    "some_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создание словарей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"\"\n",
    "path_short = \"\"\n",
    "pathtemp = \"\"\n",
    "NDOCS = 38114\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "stem = Mystem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "some_file = open(\"docs_info.pickle\", \"rb\")\n",
    "docs_info = pickle.load(some_file)\n",
    "some_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_file = open(\"headers.pickle\", \"rb\")\n",
    "headers = pickle.load(some_file)\n",
    "some_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "38114it [00:00, 483330.92it/s]\n"
     ]
    }
   ],
   "source": [
    "urls_order = defaultdict(str)\n",
    "#url_order -> url - и этот номер и есть номер в доках\n",
    "with open(path_short + \"urls.numerate.txt\", 'r') as file:\n",
    "    for _, line in enumerate(tqdm(file)):\n",
    "        line = line.strip(\"\\n\").split(\"\\t\")\n",
    "        urls_order[int(line[0])] = line[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "38772it [00:00, 287395.43it/s]\n"
     ]
    }
   ],
   "source": [
    "docs2range = defaultdict(list)\n",
    "with open(path_short + \"sample.technosphere.ir1.textrelevance.submission.txt\", 'r') as file:\n",
    "    l = file.readline()\n",
    "    for _, line in enumerate(tqdm(file)):\n",
    "        line = line.strip(\"\\n\").split(\",\")\n",
    "        docs2range[int(line[0])].append(int(line[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "alldocsNumber = 38114\n",
    "avgtitleLen = 0\n",
    "avgtextLen = 0\n",
    "avgheaderLen = 0\n",
    "titlesDict = defaultdict(int)\n",
    "textDict = defaultdict(int)\n",
    "headersDict = defaultdict(int)\n",
    "\n",
    "#8 пустых документов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 38114/38114 [06:55<00:00, 91.69it/s] \n"
     ]
    }
   ],
   "source": [
    "for key in tqdm(docs_info):\n",
    "    doc = docs_info[key]\n",
    "    \n",
    "    title = re.compile(r'[A-Za-zА-Яа-я0-9]+').findall(doc[0].lower())\n",
    "    text = re.compile(r'[A-Za-zА-Яа-я0-9]+').findall(doc[1].lower())\n",
    "    \n",
    "    avgtitleLen += len(title)\n",
    "    avgtextLen += len(text)\n",
    "    \n",
    "    met = set()\n",
    "    for word in title:\n",
    "        if word not in met:\n",
    "            titlesDict[word] += 1\n",
    "            met.add(word)\n",
    "    \n",
    "    met = set()\n",
    "    for word in text:\n",
    "        if word not in met:\n",
    "            textDict[word] += 1\n",
    "            met.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 38114/38114 [00:06<00:00, 6041.94it/s] \n"
     ]
    }
   ],
   "source": [
    "avgheaderLen = 0\n",
    "headersDict = defaultdict(int)\n",
    "for key in tqdm(headers): \n",
    "    header = re.compile(r'[A-Za-zА-Яа-я0-9]+').findall(headers[key].lower())\n",
    "    avgheaderLen += len(header)   \n",
    "    met = set()\n",
    "    for word in header:\n",
    "        if word not in met:\n",
    "            headersDict[word] += 1\n",
    "            met.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8998.754945689248, 9.041979325182348, 154.22760665372303)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avgtitleLen /= 38114\n",
    "avgtextLen /= 38114\n",
    "avgheaderLen /= 38114\n",
    "avgtextLen, avgtitleLen, avgheaderLen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36204/36204 [00:00<00:00, 158333.05it/s]\n"
     ]
    }
   ],
   "source": [
    "#pre-count idfs for every word in titledict\n",
    "import numpy as np\n",
    "IDFall = 0\n",
    "negIDF = []\n",
    "for word in tqdm(titlesDict):\n",
    "    IDF = np.log(alldocsNumber - titlesDict[word] + 0.5) - np.log(titlesDict[word] + 0.5)\n",
    "    IDFall += IDF\n",
    "    if IDF<0:\n",
    "#         IDF = eps\n",
    "        negIDF.append(word)\n",
    "    titlesDict[word] = IDF\n",
    "eps = 0.1 * IDFall / len(titlesDict)\n",
    "for word in negIDF:\n",
    "    titlesDict[word] = eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3390040/3390040 [00:14<00:00, 236731.15it/s]\n"
     ]
    }
   ],
   "source": [
    "#pre-count idfs for every word in textdict\n",
    "import numpy as np\n",
    "IDFall = 0\n",
    "negIDF = []\n",
    "for word in tqdm(textDict):\n",
    "    IDF = np.log(alldocsNumber - textDict[word] + 0.5) - np.log(textDict[word] + 0.5) #this formula?\n",
    "    IDFall += IDF\n",
    "    if IDF < 0:\n",
    "#         IDF = eps\n",
    "        negIDF.append(word)\n",
    "    textDict[word] = IDF\n",
    "eps = 0.1 * IDFall / len(textDict)\n",
    "for word in negIDF:\n",
    "    textDict[word] = eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146342/146342 [00:00<00:00, 238791.98it/s]\n"
     ]
    }
   ],
   "source": [
    "#pre-count idfs for every word in headersdict\n",
    "import numpy as np\n",
    "IDFall = 0\n",
    "negIDF = []\n",
    "for word in tqdm(headersDict):\n",
    "    IDF = np.log(alldocsNumber - headersDict[word] + 0.5) - np.log(headersDict[word] + 0.5)\n",
    "    IDFall += IDF\n",
    "    if IDF<0:\n",
    "#         IDF = eps\n",
    "        negIDF.append(word)\n",
    "    headersDict[word] = IDF\n",
    "eps = 0.1 * IDFall / len(headersDict)\n",
    "for word in negIDF:\n",
    "    headersDict[word] = eps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ранжирование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_titles_texts = []\n",
    "for key in docs_info.keys():\n",
    "    doc_titles_texts.append(docs_info[key][0].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc_texts_texts = []\n",
    "# for key in docs_info.keys():\n",
    "#     doc_texts_texts.append(docs_info[key][1].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(doc_titles_texts)]\n",
    "model = Doc2Vec(documents, vector_size=32, min_count=1, workers=10, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity_reranking(query_raw, ranked_list, k=5):\n",
    "    \n",
    "    query_raw_lemm = re.compile(r'[A-Za-zА-Яа-я0-9]+').findall(' '.join(stem.lemmatize(query_raw.lower())))\n",
    "    query_vector = model.infer_vector(query_raw_lemm)\n",
    "    \n",
    "    k = len(ranked_list)\n",
    "    \n",
    "    for i in range(k):\n",
    "        doc_ord, doc_score = ranked_list[i]\n",
    "        doc = docs_info[urls_order[doc_ord]]  \n",
    "        title_vector = model.infer_vector(doc[0].split()) #143 дока без тайтлов\n",
    "        doc_score += 30. * ( 1 - cosine(query_vector, title_vector) )\n",
    "        ranked_list[i] = tuple([doc_ord, doc_score])\n",
    "        \n",
    "    ranked_list[:k] = sorted(ranked_list[:k], key=lambda x: x[1], reverse=True)\n",
    "    return ranked_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 2.0\n",
    "b = 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_score(queryline, doc, IDFdict):\n",
    "    score = 0\n",
    "    docTF = defaultdict(float)\n",
    "    doc_s = re.compile(r'[A-Za-zА-Яа-я0-9]+').findall(doc.lower())\n",
    "    doc_len = len(doc_s)\n",
    "    query = re.compile(r'[A-Za-zА-Яа-я0-9]+').findall(' '.join(stem.lemmatize(queryline.lower())))\n",
    "    \n",
    "    for word in doc_s:\n",
    "        docTF[word] += 1\n",
    "    \n",
    "    for word in query:\n",
    "        score += IDFdict[word] * (docTF[word]*(k+1)) / (docTF[word]+k*(1-b+b*doc_len/avgtitleLen))\n",
    "    return score\n",
    "\n",
    "\n",
    "def BM25(query, docs_list):\n",
    "    scores = defaultdict(int)\n",
    "    for docOrd in docs_list:\n",
    "        score = 0\n",
    "        url = urls_order[docOrd]\n",
    "        doc = docs_info[url]\n",
    "        \n",
    "        score += 3.0 * calc_score(query, doc[0], titlesDict)\n",
    "        score += 1.2 * calc_score(query, doc[1], textDict)\n",
    "        score += 1.0 * calc_score(query, headers[url], headersDict)\n",
    "        scores[docOrd] = score\n",
    "        \n",
    "        order = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "    return order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "399it [09:34,  1.44s/it]\n"
     ]
    }
   ],
   "source": [
    "ans_csv = []\n",
    "\n",
    "query_file = \"queries.numerate_fixed_fixed.txt\" #синонимы транслиты исправлены опечатки классы запросов\n",
    "query_raw_file = \"queries.numerate_fixed.txt\" # исправлены только самые очевидные опечатки\n",
    "\n",
    "with open(query_file, 'r') as file, open(query_raw_file, 'r') as raw_file:\n",
    "    for (_,line), (_,line_raw) in tqdm(zip(enumerate(file), enumerate(raw_file))):\n",
    "    \n",
    "        split = line.strip().split('\\t')\n",
    "        split_raw = line_raw.strip().split('\\t')[1]\n",
    "        \n",
    "        docs_list = docs2range[int(split[0])]\n",
    "        ranked_list = BM25(split[1], docs_list)\n",
    "        reranked_list = cosine_similarity_reranking(split_raw, ranked_list)\n",
    "        id_list = [int(split[0])]*len(reranked_list)\n",
    "        \n",
    "        ans_csv.extend(list(zip(id_list, reranked_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, (28, 133.84481657602134)), (1, (78, 107.09481768517452)), (1, (74, 99.63557695251605)), (1, (53, 92.74555112745723)), (1, (7, 79.33425321263576)), (1, (66, 79.000205615944)), (1, (57, 73.9324903902513)), (1, (85, 72.02227326672266)), (1, (82, 69.22521949302424)), (1, (44, 67.00715222528342))]\n"
     ]
    }
   ],
   "source": [
    "print(ans_csv[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans_col = [a[1][0] for a in ans_csv]\n",
    "ans_q = [a[0] for a in ans_csv]\n",
    "ans = list(zip(ans_q, ans_col))\n",
    "\n",
    "df = pd.DataFrame(ans, columns = ['QueryId', 'DocumentId'])\n",
    "df.to_csv(\"submission21.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
